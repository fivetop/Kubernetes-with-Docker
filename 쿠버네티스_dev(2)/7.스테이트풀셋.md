## 스테이트풀셋
![화면 캡처 2021-12-02 193936](https://user-images.githubusercontent.com/62214428/144406358-8f5da613-88e0-4588-b23c-293e53fa87f4.png)
- 원래 파드를 삭제하면 그와 관련된 id,disk등 모두 함께 지워진다.
- `deployment`와 같은 급이라고 생각하면 되는데 거기서 특정 정보는 유지할 수 있는 기능이 추가된 개념
- 물론 `pod`는 삭제되는거고 `pod`정보가 유지되는 것
- `statefulSet`의 대부분의 문제는 `disk`를 유지해야해서 발생

### 1. headless service란
```
기존에는 service가 ip를 갖고 이를 dns와 연결하며, service를 통해 pod에 접근
headless에서는 서비스는 dns를 위해 만들긴 하지만
해당 service는 ip를 갖지 않으며
따라서 service를 거쳐 pod가 아닌 pod를 직접 지정하여 연결
```
![화면 캡처 2021-12-02 195322](https://user-images.githubusercontent.com/62214428/144408710-af4215a3-91d8-4220-b23a-fb81a61a7b7c.png)

### 2. statefulSet
- headless service.yaml
- `headless service`에서 주요하게 볼 부분은 역시 `clusterIp: None`
```yaml
# stateful-nginx.yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None # headless
  selector:
    app: nginx
```
- statefulSet.yaml ★
- 여기서 주로 살펴볼 부분은
- 1. `serviceName` => headless service의 이름
- 2. `terminationGracePeriodSeconds` => 우아한 종료를 위해 얼마나 기다리고 이 후 강제종료
- 3. `volumeClaimTemplates`★ 을 사용해 안정적인 스토리지 제공
    - pvclaim이라고 생각하면 된다.
- 4. storageClassName: "rook-ceph-block" => `kubectl get sc`로 storage class 존재하는지 확인
    - 나는 지금 rook-ceph-block은 지웠고 my-sc라는 sc만들어둔 상태이니 이걸 활용
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "rook-ceph-block"
      resources:
        requests:
          storage: 1Gi
```

### 3. statefulSet 전반적인 이해
- 각 파드가 pvc 요청하고 이에 따라 각각 disk를 갖는다
- 이 때 이 disk가 유지되도록 하는게 statefulset
![화면 캡처 2021-12-02 200806](https://user-images.githubusercontent.com/62214428/144410967-0b955e32-a980-4b1b-8759-7de844151e63.png)


### 4. 참고 scale 
- scale을 조정할수도 있다
```
kubectl scale statefulset web --replicas=5
kubectl scale statefulset web --replicas=1
```

### 5. 실제 statefulset을 배포하고 확인해보기
- yaml은 바로 위 yaml로 적용한다
- 특징 중 하나가 바로 질서 정연이라고 했다
![화면 캡처 2021-12-02 202234](https://user-images.githubusercontent.com/62214428/144412977-82d1dba2-5845-48f7-9530-98c34ce7d650.png)

### 6. 가장 중요한 stateful한지 확인을 해보자
- 바로 위 상황이 현재 아래 사진 상태
- ![화면 캡처 2021-12-02 202550](https://user-images.githubusercontent.com/62214428/144413411-b19129bf-5d05-42fb-beb5-dcb5d638206a.png)
- `scale`을 조정해서 몇 개 pod를 지워보고 disk를 확인해보자 / pv pvc 줄어들까? => 줄어들면 그건 stateful이 
- ![화면 캡처 2021-12-02 202734](https://user-images.githubusercontent.com/62214428/144413677-73207ee0-89b7-46da-9293-7c771a0b7956.png)
- 그리고 여기서 다시 3으로 되돌리면 위에서 보이는대로 아직남아있는 pv,pvc에 다시 생성된 pod가 붙는다


### 7. 가장 중요한게 남았다
- 앞에서 headless service를 사용한다고 했는데
- 그 말은 즉슨 pod를 직접 지정하여 연결
- 그럼 pod의 이름은 `pod이름.service이름.namespace`로 연결할 수 있다
- 그럼 여기서는 `web-0.nginx.default`인것
- 확인을 해보자
- ![화면 캡처 2021-12-02 203303](https://user-images.githubusercontent.com/62214428/144414387-e397fe87-95ab-4cc1-b5d0-9c1e8497f8d3.png)
- 접근 확인 OK
- 참고로 403은 내부에 아무 파일.. index.html 도 없어서 nginx 기본설정으로 403이 나오는것










